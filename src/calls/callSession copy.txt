import fs from 'fs';
import path from 'path';
import { env } from '../env';
import { log } from '../log';
import { describeWavHeader, parseWavInfo } from '../audio/wavInfo';
import { runPlaybackPipeline } from '../audio/playbackPipeline';
import { MediaFrame, type Pcm16Frame } from '../media/types';
import { storeWav } from '../storage/audioStore';
import {
  ChunkedSTT,
  type SpeechStartInfo,
  type STTProvider as ChunkedSttProvider,
} from '../stt/chunkedSTT';
import { getProvider } from '../stt/registry';
import { PstnTelnyxTransportSession } from '../transport/pstnTelnyxTransport';
import type { TransportSession } from '../transport/types';
import { synthesizeSpeech } from '../tts/kokoroTTS';
import { attachAudioMeta, getAudioMeta, markAudioSpan, probeWav } from '../diagnostics/audioProbe';
import type { TTSResult } from '../tts/types';
import type { RuntimeTenantConfig } from '../tenants/tenantConfig';
import { generateAssistantReply, generateAssistantReplyStream, type AssistantReplyResult } from '../ai/brainClient';
import {
  CallSessionConfig,
  CallSessionMetrics,
  CallSessionState,
  ConversationTurn,
  TranscriptBuffer,
} from './types';
import { startStageTimer, incStageError, observeStageDuration, incSttFramesFed } from '../metrics';

const PARTIAL_FAST_PATH_MIN_CHARS = 18;

function getErrorMessage(error: unknown): string {
  if (error instanceof Error) return error.message;
  return 'unknown_error';
}

function resolveDebugDir(): string {
  const dir = process.env.STT_DEBUG_DIR;
  return dir && dir.trim() !== '' ? dir.trim() : '/tmp/veralux-stt-debug';
}

function wavHeader(pcmDataBytes: number, sampleRate: number, channels: number): Buffer {
  const bytesPerSample = 2;
  const blockAlign = channels * bytesPerSample;
  const byteRate = sampleRate * blockAlign;

  const header = Buffer.alloc(44);
  header.write('RIFF', 0, 'ascii');
  header.writeUInt32LE(36 + pcmDataBytes, 4);
  header.write('WAVE', 8, 'ascii');
  header.write('fmt ', 12, 'ascii');
  header.writeUInt32LE(16, 16);
  header.writeUInt16LE(1, 20);
  header.writeUInt16LE(channels, 22);
  header.writeUInt32LE(sampleRate, 24);
  header.writeUInt32LE(byteRate, 28);
  header.writeUInt16LE(blockAlign, 32);
  header.writeUInt16LE(16, 34);
  header.write('data', 36, 'ascii');
  header.writeUInt32LE(pcmDataBytes, 40);
  return header;
}

function encodePcm16Wav(pcm16le: Buffer, sampleRateHz: number): Buffer {
  const header = wavHeader(pcm16le.length, sampleRateHz, 1);
  return Buffer.concat([header, pcm16le]);
}

export class CallSession {
  public readonly callControlId: string;
  public readonly tenantId?: string;
  public readonly from?: string;
  public readonly to?: string;
  public readonly requestId?: string;

  private state: CallSessionState = 'INIT';
  private readonly transcriptBuffer: TranscriptBuffer = [];
  private readonly conversationHistory: ConversationTurn[] = [];
  private readonly metrics: CallSessionMetrics;
  private readonly stt: ChunkedSTT;
  private readonly transport: TransportSession;
  private readonly logContext: Record<string, unknown>;
  private readonly deadAirMs = env.DEAD_AIR_MS;
  private readonly deadAirNoFramesMs = env.DEAD_AIR_NO_FRAMES_MS;
  private readonly rxSampleRateHz: number;
  private readonly sttConfig?: RuntimeTenantConfig['stt'];
  private readonly ttsConfig?: RuntimeTenantConfig['tts'];
  private endedAt?: number;
  private endedReason?: string;
  private active = true;

  private sttInFlightCount = 0;
  private isHandlingTranscript = false;
  private hasStarted = false;
  private turnSequence = 0;
  private deadAirTimer?: NodeJS.Timeout;
  private repromptInFlight = false;
  private ingestFailurePrompted = false;
  private readonly logPreviewChars = 160;
  private ttsSegmentChain: Promise<void> = Promise.resolve();
  private ttsSegmentQueueDepth = 0;
  private playbackState: { active: boolean; interrupted: boolean; segmentId?: string } = {
    active: false,
    interrupted: false,
  };
  private playbackStopSignal?: { promise: Promise<void>; resolve: () => void };
  private transcriptHandlingToken = 0;
  private transcriptAcceptedForUtterance = false;
  private deferredTranscript?: { text: string; source?: 'partial_fallback' | 'final' };
  private firstPartialAt?: number;
  private lastSpeechStartAtMs = 0;
  private lastDecodedFrameAtMs = 0;
  private rxDumpActive = false;
  private rxDumpSamplesTarget = 0;
  private rxDumpSamplesCollected = 0;
  private rxDumpBuffers: Buffer[] = [];
  private listeningSinceAtMs = 0;
  // pick reasonable defaults; you can env-ize later
  private readonly deadAirListeningGraceMs = 1200;  // prevents immediate reprompt right after enter LISTENING
  private readonly deadAirAfterSpeechStartGraceMs = 1500; // prevents reprompt while user has started speaking but transcript not ready
  // ===== STT in-flight tracking (prevents dead-air reprompt while Whisper HTTP is running) =====
  private onSttRequestStart(kind: 'partial' | 'final'): void {
    this.sttInFlightCount += 1;
    log.info(
      { event: 'stt_req_start', kind, in_flight: this.sttInFlightCount, ...this.logContext },
      'stt request started',
    );
  }

  private onSttRequestEnd(kind: 'partial' | 'final'): void {
    this.sttInFlightCount = Math.max(0, this.sttInFlightCount - 1);
    log.info(
      { event: 'stt_req_end', kind, in_flight: this.sttInFlightCount, ...this.logContext },
      'stt request ended',
    );
  }


  constructor(config: CallSessionConfig) {
    this.callControlId = config.callControlId;
    this.tenantId = config.tenantId;
    this.from = config.from;
    this.to = config.to;
    this.requestId = config.requestId;

    this.metrics = {
      createdAt: new Date(),
      lastHeardAt: undefined,
      turns: 0,
    };

    this.sttConfig = config.tenantConfig?.stt;
    this.ttsConfig = config.tenantConfig?.tts;

    this.logContext = {
      call_control_id: this.callControlId,
      tenant_id: this.tenantId,
      requestId: this.requestId,
      telnyx_track: env.TELNYX_STREAM_TRACK,
    };

    this.transport =
      config.transportSession ??
      new PstnTelnyxTransportSession({
        callControlId: this.callControlId,
        tenantId: this.tenantId,
        requestId: this.requestId,
        isActive: () => this.active && this.state !== 'ENDED',
      });

    // ✅ Ensure this is ALWAYS a string (tenant override → env fallback)
    const sttEndpointUrl =
      this.sttConfig?.config?.url ??
      this.sttConfig?.whisperUrl ??
      env.WHISPER_URL;

    const sttMode = this.sttConfig?.mode ?? 'whisper_http';
    const provider = getProvider(sttMode) as unknown as ChunkedSttProvider;
    const selectedMode =
      sttMode === 'http_wav_json' && !env.ALLOW_HTTP_WAV_JSON ? 'whisper_http' : sttMode;
    log.info(
      {
        event: 'stt_provider_selected',
        call_control_id: this.callControlId,
        stt_mode: selectedMode,
        requested_mode: sttMode,
        provider_id: provider.id,
        ...(this.logContext ?? {}),
      },
      'stt provider selected',
    );
    const sttAudioInput =
      this.transport.mode === 'pstn'
        ? { codec: 'pcm16le' as const, sampleRateHz: env.TELNYX_TARGET_SAMPLE_RATE }
        : this.transport.audioInput;
    this.rxSampleRateHz = sttAudioInput.sampleRateHz;

    this.stt = new ChunkedSTT({
      provider,
      whisperUrl: sttEndpointUrl,
      language: this.sttConfig?.language,
      frameMs: this.sttConfig?.chunkMs ?? env.STT_CHUNK_MS,
      silenceEndMs: env.STT_SILENCE_MS,
      inputCodec: sttAudioInput.codec,
      sampleRate: sttAudioInput.sampleRateHz,
      onTranscript: async (text, source) => {
        await this.handleTranscript(text, source);
      },
      onSpeechStart: (info: SpeechStartInfo) => {
        void this.handleSpeechStart(info);
      },
      // ✅ STT in-flight hooks (ChunkedSTT calls these when provider requests start/end)
      onSttRequestStart: (kind) => this.onSttRequestStart(kind),
      onSttRequestEnd: (kind) => this.onSttRequestEnd(kind),

      isPlaybackActive: () => this.isPlaybackActive(),
      isListening: () => this.isListening(),
      getTrack: () => env.TELNYX_STREAM_TRACK,
      getCodec: () => this.transport.audioInput.codec,
      logContext: this.logContext,
    });
  }


  public start(options: { autoAnswer?: boolean } = {}): boolean {
    if (!this.active || this.state === 'ENDED' || this.hasStarted) {
      return false;
    }

    this.state = 'INIT';
    this.hasStarted = true;

    if (options.autoAnswer !== false) {
      void this.answerAndGreet();
    }

    return true;
  }

  public onAnswered(): boolean {
    if (!this.active || this.state === 'ENDED') {
      return false;
    }

    const previousState = this.state;
    if (this.state === 'INIT') {
      this.state = 'ANSWERED';
    }

    this.metrics.lastHeardAt = new Date();
    return previousState !== this.state;
  }

  public onAudioFrame(frame: MediaFrame): void {
    if (!this.active || this.state === 'ENDED') return;

    // PSTN must feed STT with decoded PCM16 only (via onPcm16Frame)
    if (this.transport.mode === 'pstn') {
      log.warn(
        { event: 'unexpected_audio_frame_on_pstn', ...this.logContext },
        'PSTN transport should call onPcm16Frame (decoded pcm16) not onAudioFrame',
      );
      return;
    }

    // Non-PSTN / WebRTC can continue using this path if that's how your transport works.
    this.lastDecodedFrameAtMs = Date.now();

    if (this.state === 'INIT' || this.state === 'ANSWERED') {
      this.enterListeningState();
    } else if (this.state === 'LISTENING') {
      this.scheduleDeadAirTimer();
    }

    this.metrics.lastHeardAt = new Date();
    incSttFramesFed();

    // IMPORTANT: only do rx dump here if you KNOW these bytes are pcm16.
    // If you don't, remove this line entirely.
    // this.maybeCaptureRxDump(frame as unknown as Buffer);

    this.stt.ingest(frame);
  }


  public onPcm16Frame(frame: Pcm16Frame): void {
    if (!this.active || this.state === 'ENDED') {
      return;
    }

    const now = Date.now();

    // “We received inbound audio” marker (even if STT is gated during playback)
    this.lastDecodedFrameAtMs = now;
    this.metrics.lastHeardAt = new Date();

    // State transitions + dead-air arming must happen for every inbound frame
    if (this.state === 'INIT' || this.state === 'ANSWERED') {
      this.enterListeningState();
    } else if (this.state === 'LISTENING') {
      // ✅ CRITICAL: keep dead-air timer fresh while listening
      this.scheduleDeadAirTimer();
    }

    if (frame.sampleRateHz !== this.rxSampleRateHz) {
      log.warn(
        {
          event: 'stt_sample_rate_mismatch',
          expected_hz: this.rxSampleRateHz,
          got_hz: frame.sampleRateHz,
          ...this.logContext,
        },
        'stt sample rate mismatch',
      );
    }

    const pcmBuffer = Buffer.from(frame.pcm16.buffer, frame.pcm16.byteOffset, frame.pcm16.byteLength);

    incSttFramesFed();
    this.maybeCaptureRxDump(pcmBuffer);


    this.stt.ingestPcm16(frame.pcm16, frame.sampleRateHz);
  }

  public isPlaybackActive(): boolean {
    if (!this.active || this.state === 'ENDED') {
      return false;
    }
    // ✅ playback is active if the playback flag is set OR streaming segments are queued
    return this.playbackState.active || this.ttsSegmentQueueDepth > 0;
  }


  public isListening(): boolean {
    return this.state === 'LISTENING';
  }

  public getLastSpeechStartAtMs(): number {
    return this.lastSpeechStartAtMs;
  }

  public notifyIngestFailure(reason: string): void {
    if (!this.active || this.state === 'ENDED') {
      return;
    }
    if (this.ingestFailurePrompted || this.repromptInFlight) {
      return;
    }

    this.ingestFailurePrompted = true;
    this.repromptInFlight = true;
    this.stt.stop();

    const turnId = `ingest-${this.nextTurnId()}`;
    log.warn(
      { event: 'call_session_ingest_failure_prompt', reason, ...this.logContext },
      'ingest failure prompt',
    );

    void this.playText("I'm having trouble hearing you. Please try again.", turnId)
      .catch((error) => {
        log.warn({ err: error, ...this.logContext }, 'ingest failure reprompt failed');
      })
      .finally(() => {
        this.repromptInFlight = false;
        if (this.state === 'LISTENING') {
          this.scheduleDeadAirTimer();
        }
      });
  }

  public end(): boolean {
    if (this.state === 'ENDED') {
      this.markEnded('ended');
      return false;
    }

    this.markEnded('ended');
    this.state = 'ENDED';
    this.metrics.lastHeardAt = new Date();
    this.clearDeadAirTimer();
    this.stt.stop();
    return true;
  }

  public getState(): CallSessionState {
    return this.state;
  }

  public getTransport(): TransportSession {
    return this.transport;
  }

  public isActive(): boolean {
    return this.active;
  }

  public markEnded(reason: string): void {
    if (!this.active) {
      if (!this.endedReason) {
        this.endedReason = reason;
      }
      return;
    }

    this.active = false;
    this.endedAt = Date.now();
    this.endedReason = reason;
    log.info(
      { event: 'call_marked_inactive', reason, ...this.logContext },
      'call marked inactive',
    );
  }

  public getEndInfo(): { endedAt?: number; endedReason?: string } {
    return {
      endedAt: this.endedAt,
      endedReason: this.endedReason,
    };
  }

  public getMetrics(): CallSessionMetrics {
    return {
      createdAt: new Date(this.metrics.createdAt),
      lastHeardAt: this.metrics.lastHeardAt ? new Date(this.metrics.lastHeardAt) : undefined,
      turns: this.metrics.turns,
    };
  }

  public getLastActivityAt(): Date {
    return this.metrics.lastHeardAt ?? this.metrics.createdAt;
  }

  public appendTranscriptSegment(segment: string): void {
    if (segment.trim() === '') {
      return;
    }
    this.transcriptBuffer.push(segment);
  }

  public appendHistory(turn: ConversationTurn): void {
    this.conversationHistory.push(turn);
    this.metrics.turns += 1;
  }

  public onPlaybackEnded(): void {
    if (!this.playbackState.active) {
      return;
    }

    // ✅ If streaming segments are still queued, do NOT end playback yet.
    // The segment queue's .finally() will call onPlaybackEnded() when depth hits 0.
    if (this.ttsSegmentQueueDepth > 0 && !this.playbackState.interrupted) {
      return;
    }

    const wasInterrupted = this.playbackState.interrupted;


    // ✅ ALWAYS clear playback flags FIRST
    this.playbackState.active = false;
    this.playbackState.interrupted = false;
    this.playbackState.segmentId = undefined;

    // ✅ resolve + clear stop signal (don’t just null it)
    this.resolvePlaybackStopSignal();
    this.playbackStopSignal = undefined;
    

    // ✅ do NOT force queue depth to 0 here.
    // Queue depth drains naturally; onPlaybackEnded() is called when it reaches 0.
    // Only clear depth explicitly during barge-in / queue cancel paths.


    if (wasInterrupted) {
      log.info(
        { event: 'playback_ended_after_barge_in', ...this.logContext },
        'playback ended after barge-in',
      );

      // ✅ exit SPEAKING so isPlaybackActive() becomes false
      if (this.active && this.state !== 'ENDED') {
        this.enterListeningState(true); // barge-in: arm timer normally
      }

      this.startRxDumpAfterPlayback();
      return;
    }

    // ✅ normal playback end: enter LISTENING but DON'T arm dead-air yet
    if (this.active && this.state === 'SPEAKING') {
      this.enterListeningState(false);
    }

    // ✅ if we have a deferred FINAL, consume it immediately (no reprompt racing)
    if (this.active && this.state === 'LISTENING') {
      this.flushDeferredTranscript();

      // ✅ only arm dead-air if we are still listening and not handling a transcript
      if (!this.isHandlingTranscript && this.state === 'LISTENING') {
        this.scheduleDeadAirTimer();
      }
    }

    this.startRxDumpAfterPlayback();

  }


  private createPlaybackStopSignal(): { promise: Promise<void>; resolve: () => void } {
    let resolve: () => void;
    const promise = new Promise<void>((resolver) => {
      resolve = resolver;
    });
    return { promise, resolve: resolve! };
  }

  private beginPlayback(segmentId?: string): void {
    if (!this.playbackState.active) {
      this.playbackStopSignal = this.createPlaybackStopSignal();
    }
    this.playbackState.active = true;
    this.playbackState.interrupted = false;
    this.playbackState.segmentId = segmentId;
    this.state = 'SPEAKING';
    this.clearDeadAirTimer();
    this.resetRxDump();
  }

  private resolvePlaybackStopSignal(): void {
    if (this.playbackStopSignal) {
      this.playbackStopSignal.resolve();
      this.playbackStopSignal = undefined;
    }
  }

  private clearTtsQueue(): void {
    this.ttsSegmentChain = Promise.resolve();
    this.ttsSegmentQueueDepth = 0;
  }

  private invalidateTranscriptHandling(): void {
    this.transcriptHandlingToken += 1;
    this.isHandlingTranscript = false;
  }

  private flushDeferredTranscript(): void {
    if (!this.deferredTranscript) {
      return;
    }
    if (!this.active || this.state !== 'LISTENING' || this.isHandlingTranscript) {
      return;
    }

    const deferred = this.deferredTranscript;
    this.deferredTranscript = undefined;
    void this.handleTranscript(deferred.text, deferred.source);
  }

  private logTtsBytesReady(
    id: string,
    audio: Buffer,
    contentType: string | undefined,
  ): void {
    const header = describeWavHeader(audio);
    log.info(
      {
        event: 'tts_bytes_ready',
        id,
        bytes: audio.length,
        riff: header.riff,
        wave: header.wave,
        ...this.logContext,
      },
      'tts bytes ready',
    );

    if (!header.riff || !header.wave) {
      log.warn(
        {
          event: 'tts_non_wav_warning',
          id,
          content_type: contentType,
          first16_hex: header.first16Hex,
          bytes: audio.length,
          ...this.logContext,
        },
        'tts bytes are not wav',
      );
    }

    const audioLogContext = { ...this.logContext, tts_id: id };
    const baseMeta = {
      callId: this.callControlId,
      tenantId: this.tenantId,
      format: 'wav' as const,
      logContext: audioLogContext,
      lineage: ['tts:output'],
      kind: id,
    };
    attachAudioMeta(audio, baseMeta);
    probeWav('tts.out.raw', audio, baseMeta);

    this.logWavInfo('kokoro', id, audio);
  }

  private logWavInfo(source: 'kokoro' | 'pipeline_output', id: string, audio: Buffer): void {
    try {
      const info = parseWavInfo(audio);
      log.info(
        {
          event: 'wav_info',
          source,
          id,
          sample_rate_hz: info.sampleRateHz,
          channels: info.channels,
          bits_per_sample: info.bitsPerSample,
          data_bytes: info.dataBytes,
          duration_ms: info.durationMs,
          ...this.logContext,
        },
        'wav info',
      );
    } catch (error) {
      log.warn(
        {
          event: 'wav_info_parse_failed',
          source,
          id,
          reason: getErrorMessage(error),
          ...this.logContext,
        },
        'wav info parse failed',
      );
    }
  }

  private resetTranscriptTracking(): void {
    this.transcriptAcceptedForUtterance = false;
    this.deferredTranscript = undefined;
    this.firstPartialAt = undefined;
  }

  private shouldTriggerPartialFastPath(text: string): boolean {
    const trimmed = text.trim();
    if (!trimmed) return false;
    if (/[.!?]$/.test(trimmed)) return true;
    return trimmed.length >= PARTIAL_FAST_PATH_MIN_CHARS;
  }

  private handleSpeechStart(info: SpeechStartInfo): void {
    if (!this.active || this.state === 'ENDED') {
      return;
    }

    this.lastSpeechStartAtMs = Date.now();
    this.resetTranscriptTracking();

    const playbackActive = this.isPlaybackActive();
    if (!playbackActive || this.playbackState.interrupted) {
      return;
    }

    log.info(
      {
        event: 'barge_in',
        reason: 'speech_start',
        state: this.state,
        speech_rms: info.rms,
        speech_peak: info.peak,
        speech_frame_ms: Math.round(info.frameMs),
        speech_frame_streak: info.streak,
        ...this.logContext,
      },
      'barge in',
    );

    // ✅ mark interrupted, but DO NOT clear active here.
    // onPlaybackEnded() needs active=true to run its cleanup.
    this.playbackState.interrupted = true;
    this.playbackState.segmentId = undefined;

    this.resolvePlaybackStopSignal();

    // ✅ Cancel queued segments without double-manipulating the counter
    // clearTtsQueue() already sets queue depth to 0.
    this.clearTtsQueue();
    this.invalidateTranscriptHandling();

    // ✅ Stop playback first; onPlaybackEnded() will re-enter LISTENING and start rx dump.
    // Avoid double transitions + weird flush timing.
    void this.stopPlayback();


  }

  private async stopPlayback(): Promise<void> {
    try {
      await this.transport.playback.stop();
    } catch (error) {
      log.warn({ err: error, ...this.logContext }, 'playback stop failed');
    } finally {
      // ✅ even if stop() fails, ensure we clear speaking/playback state
      this.onPlaybackEnded();
    }
  }


  private enterListeningState(armDeadAir: boolean = true): void {
    if (!this.active || this.state === 'ENDED') {
      return;
    }

    this.state = 'LISTENING';
    this.listeningSinceAtMs = Date.now();

    if (armDeadAir) {
      this.scheduleDeadAirTimer();
    }
  }



  private scheduleDeadAirTimer(): void {
    if (!this.active || this.state !== 'LISTENING') {
      return;
    }

    this.clearDeadAirTimer();
    this.deadAirTimer = setTimeout(() => {
      void this.handleDeadAirTimeout();
    }, this.deadAirMs);
    this.deadAirTimer.unref?.();
  }

  private clearDeadAirTimer(): void {
    if (this.deadAirTimer) {
      clearTimeout(this.deadAirTimer);
      this.deadAirTimer = undefined;
    }
  }

  private startRxDumpAfterPlayback(): void {
    if (!env.STT_DEBUG_DUMP_RX_WAV) {
      return;
    }
    this.rxDumpActive = true;
    this.rxDumpSamplesTarget = Math.max(1, Math.round(this.rxSampleRateHz * 2));
    this.rxDumpSamplesCollected = 0;
    this.rxDumpBuffers = [];
  }

  private resetRxDump(): void {
    this.rxDumpActive = false;
    this.rxDumpSamplesCollected = 0;
    this.rxDumpSamplesTarget = 0;
    this.rxDumpBuffers = [];
  }

  private maybeCaptureRxDump(frame: Buffer): void {
    if (!this.rxDumpActive) {
      return;
    }
    const sampleCount = Math.floor(frame.length / 2);
    if (sampleCount <= 0) {
      return;
    }
    this.rxDumpBuffers.push(Buffer.from(frame));
    this.rxDumpSamplesCollected += sampleCount;
    if (this.rxDumpSamplesCollected >= this.rxDumpSamplesTarget) {
      void this.flushRxDump();
    }
  }

  private async flushRxDump(): Promise<void> {
    if (!this.rxDumpActive) {
      return;
    }
    this.rxDumpActive = false;

    const pcmBuffer = Buffer.concat(this.rxDumpBuffers);
    this.rxDumpBuffers = [];

    if (pcmBuffer.length === 0) {
      return;
    }

    const dir = resolveDebugDir();
    const filePath = path.join(
      dir,
      `rx_after_playback_${this.callControlId}_${Date.now()}.wav`,
    );

    try {
      await fs.promises.mkdir(dir, { recursive: true });
      const wav = encodePcm16Wav(pcmBuffer, this.rxSampleRateHz);
      await fs.promises.writeFile(filePath, wav);
      log.info(
        {
          event: 'stt_debug_rx_wav_written',
          file_path: filePath,
          sample_rate_hz: this.rxSampleRateHz,
          bytes: wav.length,
          ...this.logContext,
        },
        'stt debug rx wav written',
      );
    } catch (error) {
      log.warn(
        { err: error, file_path: filePath, ...this.logContext },
        'stt debug rx wav write failed',
      );
    }
  }

  private async handleDeadAirTimeout(): Promise<void> {
    if (!this.active || this.state !== 'LISTENING' || this.repromptInFlight) {
      return;
    }

    // If STT is running / request in flight, don't reprompt.
    // (Stronger than isHandlingTranscript. Keep both if you want.)
    if (this.sttInFlightCount && this.sttInFlightCount > 0) {
      this.scheduleDeadAirTimer();
      return;
    }

    if (this.isHandlingTranscript) {
      this.scheduleDeadAirTimer();
      return;
    }

    const now = Date.now();

    // 1) Grace right after we enter LISTENING
    if (this.listeningSinceAtMs > 0 && now - this.listeningSinceAtMs < this.deadAirListeningGraceMs) {
      this.scheduleDeadAirTimer();
      return;
    }

    // 2) Grace after speech start (STT might be behind)
    if (this.lastSpeechStartAtMs > 0 && now - this.lastSpeechStartAtMs < this.deadAirAfterSpeechStartGraceMs) {
      this.scheduleDeadAirTimer();
      return;
    }

    // 3) If we recently received frames, don't reprompt
    if (this.lastDecodedFrameAtMs > 0 && now - this.lastDecodedFrameAtMs < this.deadAirNoFramesMs) {
      this.scheduleDeadAirTimer();
      return;
    }

    // 3b) If we haven't received any frame since entering LISTENING, don't reprompt
    if (!this.lastDecodedFrameAtMs || (this.listeningSinceAtMs > 0 && this.lastDecodedFrameAtMs < this.listeningSinceAtMs)) {
      this.scheduleDeadAirTimer();
      return;
    }

    // 4) Never reprompt during playback/tts
    if (this.isPlaybackActive()) {
      this.scheduleDeadAirTimer();
      return;
    }

    this.repromptInFlight = true;
    try {
      await this.playText('Are you still there?', `reprompt-${this.nextTurnId()}`);
      log.info({ event: 'call_session_reprompt', ...this.logContext }, 'dead air reprompt');
    } finally {
      this.repromptInFlight = false;
      if (this.state === 'LISTENING') {
        this.listeningSinceAtMs = Date.now();
        this.scheduleDeadAirTimer();
      }
    }
  }



  private async handleTranscript(
    text: string,
    transcriptSource?: 'partial_fallback' | 'final',
  ): Promise<void> {
    if (!this.active || this.state === 'ENDED' || this.isHandlingTranscript) {
      return;
    }

    const trimmed = text.trim();
    if (trimmed === '') {
      return;
    }

    const isPartial = transcriptSource === 'partial_fallback';
    const trigger = isPartial ? 'partial' : 'final';

    // If we've already accepted a transcript for this utterance, ignore anything else.
    if (this.transcriptAcceptedForUtterance) {
      return;
    }

    // ===== CHANGE #1 (CORE FIX): partials DO NOT trigger a turn =====
    // We only buffer partials for debugging/visibility. The agent reply + TTS is final-only.
    if (isPartial) {
      if (!this.firstPartialAt) {
        this.firstPartialAt = Date.now();
      }

      // Keep the latest partial around (useful for debugging and optional future fallback logic)
      this.deferredTranscript = { text: trimmed, source: 'partial_fallback' };

      const partialPreview =
        trimmed.length <= this.logPreviewChars
          ? trimmed
          : `${trimmed.slice(0, this.logPreviewChars - 3)}...`;

      log.info(
        {
          event: 'partial_buffered_no_turn',
          trigger: 'partial',
          transcript_length: trimmed.length,
          transcript_preview: partialPreview,
          state: this.state,
          ...this.logContext,
        },
        'partial buffered (final-only turn policy)',
      );

      // IMPORTANT: do not set transcriptAcceptedForUtterance here.
      return;
    }

    // ===== From here on: FINAL ONLY =====

    // If playback is active and not interrupted, defer the FINAL until playback ends.
    const playbackActive = this.isPlaybackActive();
    if (playbackActive && !this.playbackState.interrupted) {
      this.deferredTranscript = { text: trimmed, source: 'final' };

      log.info(
        {
          event: 'transcript_deferred_playback',
          trigger: 'final',
          transcript_length: trimmed.length,
          state: this.state,
          playback_active: this.playbackState.active,
          tts_queue_depth: this.ttsSegmentQueueDepth,
          ...this.logContext,
        },
        'final transcript deferred during playback',
      );
      return;
    }

    const tenantLabel = this.tenantId ?? 'unknown';
    const responseStartAt = Date.now();

    // timing metric: if we had partials, measure partial->response
    if (this.firstPartialAt) {
      observeStageDuration(
        'stt_first_partial_to_response_ms',
        tenantLabel,
        responseStartAt - this.firstPartialAt,
      );
    } else {
      observeStageDuration('stt_final_to_response_ms', tenantLabel, 0);
    }

    log.info(
      {
        event: 'turn_trigger',
        trigger: 'final',
        transcript_length: trimmed.length,
        ...this.logContext,
      },
      'turn trigger',
    );

    // Accept this FINAL as the utterance we will respond to.
    this.transcriptAcceptedForUtterance = true;
    this.isHandlingTranscript = true;
    const handlingToken = (this.transcriptHandlingToken += 1);
    this.clearDeadAirTimer();

    try {
      const transcriptPreview =
        trimmed.length <= this.logPreviewChars
          ? trimmed
          : `${trimmed.slice(0, this.logPreviewChars - 3)}...`;

      log.info(
        {
          event: 'transcript_received',
          transcript_length: trimmed.length,
          transcript_preview: transcriptPreview,
          ...this.logContext,
        },
        'final transcript received',
      );

      this.state = 'THINKING';
      this.appendTranscriptSegment(trimmed);
      this.appendHistory({ role: 'user', content: trimmed, timestamp: new Date() });

      let response = '';
      let replySource = 'unknown';
      let playbackDone: Promise<void> | undefined;

      try {
        if (env.BRAIN_STREAMING_ENABLED) {
          const streamResult = await this.streamAssistantReply(trimmed, handlingToken);
          response = streamResult.reply.text;
          replySource = streamResult.reply.source;
          playbackDone = streamResult.playbackDone;
        } else {
          const endLlm = startStageTimer('llm', tenantLabel);
          try {
            const reply = await generateAssistantReply({
              tenantId: this.tenantId,
              callControlId: this.callControlId,
              transcript: trimmed,
              history: this.conversationHistory,
            });
            endLlm();
            response = reply.text;
            replySource = reply.source;
          } catch (error) {
            incStageError('llm', tenantLabel);
            endLlm();
            throw error;
          }
        }
      } catch (error) {
        response = 'Acknowledged.';
        replySource = 'fallback_error';
        log.error(
          { err: error, assistant_reply_source: replySource, ...this.logContext },
          'assistant reply generation failed',
        );
      }

      if (handlingToken !== this.transcriptHandlingToken) {
        return;
      }

      markAudioSpan('llm_result', {
        callId: this.callControlId,
        tenantId: this.tenantId,
        logContext: this.logContext,
      });

      const replyPreview =
        response.length <= this.logPreviewChars
          ? response
          : `${response.slice(0, this.logPreviewChars - 3)}...`;

      log.info(
        {
          event: 'assistant_reply_text',
          assistant_reply_text: replyPreview,
          assistant_reply_length: response.length,
          assistant_reply_source: replySource,
          ...this.logContext,
        },
        'assistant reply text',
      );

      if (handlingToken !== this.transcriptHandlingToken) {
        return;
      }

      this.appendHistory({ role: 'assistant', content: response, timestamp: new Date() });

      if (env.BRAIN_STREAMING_ENABLED) {
        if (playbackDone) {
          await playbackDone;
        }
      } else {
        await this.playAssistantTurn(response);
      }
    } catch (error) {
      log.error({ err: error, ...this.logContext }, 'call session transcript handling failed');
    } finally {
      if (handlingToken === this.transcriptHandlingToken) {
        // ✅ Reset utterance gating so next user turn isn't ignored
        this.resetTranscriptTracking();

        // reset handling flags and go back to listening
        this.isHandlingTranscript = false;
        this.listeningSinceAtMs = Date.now();
        this.enterListeningState(true);
      }
    }
  }



  private async streamAssistantReply(
    transcript: string,
    handlingToken: number,
  ): Promise<{ reply: AssistantReplyResult; playbackDone?: Promise<void> }> {
    let bufferedText = '';
    let firstTokenAt: number | undefined;
    let speakCursor = 0;
    let firstSegmentQueued = false;
    let segmentIndex = 0;
    let queuedSegments = 0;
    let baseTurnId: string | undefined;
    const firstSegmentMin = env.BRAIN_STREAM_SEGMENT_MIN_CHARS;
    const nextSegmentMin = env.BRAIN_STREAM_SEGMENT_NEXT_CHARS;
    const firstAudioMaxMs = env.BRAIN_STREAM_FIRST_AUDIO_MAX_MS;

    const queueSegment = (segment: string): void => {
      if (handlingToken !== this.transcriptHandlingToken) return;

      const trimmed = segment.trim();
      if (!trimmed) return;

      const resolvedTurnId = baseTurnId ?? `turn-${this.nextTurnId()}`;
      baseTurnId = resolvedTurnId;

      segmentIndex += 1;
      queuedSegments += 1; // ✅ FIX: count queued segments

      const segmentId = `${resolvedTurnId}-${segmentIndex}`;
      this.queueTtsSegment(trimmed, segmentId, handlingToken);
    };


    const maybeQueueSegments = (force: boolean): void => {
      if (!this.active) {
        return;
      }

      while (true) {
        const pending = bufferedText.slice(speakCursor);
        if (!pending) {
          return;
        }

        if (!firstSegmentQueued) {
          const boundary = this.findSentenceBoundary(pending);
          if (boundary !== null) {
            queueSegment(pending.slice(0, boundary));
            speakCursor += boundary;
            firstSegmentQueued = true;
            continue;
          }

          if (pending.length >= firstSegmentMin) {
            const end = this.selectSegmentEnd(pending, firstSegmentMin);
            queueSegment(pending.slice(0, end));
            speakCursor += end;
            firstSegmentQueued = true;
            continue;
          }

          if (
            force ||
            (firstTokenAt && Date.now() - firstTokenAt >= firstAudioMaxMs)
          ) {
            queueSegment(pending);
            speakCursor += pending.length;
            firstSegmentQueued = true;
            continue;
          }

          return;
        }

        const boundary = this.findSentenceBoundary(pending);
        if (boundary !== null) {
          queueSegment(pending.slice(0, boundary));
          speakCursor += boundary;
          continue;
        }

        if (pending.length >= nextSegmentMin) {
          const end = this.selectSegmentEnd(pending, nextSegmentMin);
          queueSegment(pending.slice(0, end));
          speakCursor += end;
          continue;
        }

        if (force) {
          queueSegment(pending);
          speakCursor += pending.length;
        }
        return;
      }
    };

    const tenantLabel = this.tenantId ?? 'unknown';
    const endLlm = startStageTimer('llm', tenantLabel);

    let reply: AssistantReplyResult;
    try {
      reply = await generateAssistantReplyStream(
        {
          tenantId: this.tenantId,
          callControlId: this.callControlId,
          transcript,
          history: this.conversationHistory,
        },
        (chunk) => {
          if (!chunk) return;
          if (!firstTokenAt) firstTokenAt = Date.now();
          bufferedText += chunk;
          maybeQueueSegments(false);
        },
      );
      endLlm();
    } catch (error) {
      incStageError('llm', tenantLabel);
      endLlm();
      throw error;
    }


    if (handlingToken !== this.transcriptHandlingToken) {
      return { reply };
    }

    if (reply.source !== 'brain_http_stream') {
      if (handlingToken !== this.transcriptHandlingToken) {
        return { reply };
      }
      return { reply, playbackDone: this.playAssistantTurn(reply.text) };
    }

    if (reply.text.length > bufferedText.length) {
      bufferedText = reply.text;
    }
    maybeQueueSegments(true);

    if (queuedSegments === 0) {
      if (handlingToken !== this.transcriptHandlingToken) {
        return { reply };
      }
      return { reply, playbackDone: this.playAssistantTurn(reply.text) };
    }

    return { reply, playbackDone: this.waitForTtsSegmentQueue() };
  }

  private async answerAndGreet(): Promise<void> {
    try {
      const answerStarted = Date.now();
      if (this.transport.mode === 'pstn' && this.shouldSkipTelnyxAction('answer')) {
        return;
      }
      await this.transport.start();
      const answerDuration = Date.now() - answerStarted;

      if (this.transport.mode === 'pstn') {
        log.info(
          { event: 'telnyx_answer_duration', duration_ms: answerDuration, ...this.logContext },
          'telnyx answer completed',
        );
      }
      log.info({ event: 'call_answered', ...this.logContext }, 'call answered');

      this.onAnswered();

      if (this.transport.mode === 'webrtc_hd') {
        await this.playText('Hi! Thanks for calling. How can I help you today?', 'greeting');
        return;
      }

      const trimmedBaseUrl = env.AUDIO_PUBLIC_BASE_URL.replace(/\/$/, '');
      const greetingUrl = `${trimmedBaseUrl}/greeting.wav`;

      if (this.shouldSkipTelnyxAction('playback_start')) {
        return;
      }
      this.beginPlayback('greeting');
      try {
        await this.transport.playback.play({ kind: 'url', url: greetingUrl });
        this.onPlaybackEnded();
      } catch (error) {
        this.onPlaybackEnded();
        throw error;
      }

      log.info(
        { event: 'call_playback_started', audio_url: greetingUrl, ...this.logContext },
        'playback started',
      );
    } catch (error) {
      log.error({ err: error, ...this.logContext }, 'call start greeting failed');
    }
  }

  private async playAssistantTurn(text: string): Promise<void> {
    const turnId = `turn-${this.nextTurnId()}`;
    await this.playText(text, turnId);
  }

  private async playText(text: string, turnId: string): Promise<void> {
    if (!this.active || this.state === 'ENDED') {
      return;
    }

    this.beginPlayback(turnId);

    try {
      const tenantLabel = this.tenantId ?? 'unknown';
      const endTts = startStageTimer('tts', tenantLabel);

      const spanMeta = {
        callId: this.callControlId,
        tenantId: this.tenantId,
        logContext: { ...this.logContext, tts_id: turnId },
        kind: turnId,
      };
      markAudioSpan('tts_start', spanMeta);
      const ttsStart = Date.now();
      let result: TTSResult;
      try {
        result = await synthesizeSpeech({
          text,
          voice: this.ttsConfig?.voice,
          format: this.ttsConfig?.format,
          sampleRate: this.ttsConfig?.sampleRate,
          kokoroUrl: this.ttsConfig?.kokoroUrl,
        });
      } catch (error) {
        incStageError('tts', tenantLabel);
        throw error;
      } finally {
        endTts();
      }

      const ttsDuration = Date.now() - ttsStart;
      markAudioSpan('tts_ready', spanMeta);

      log.info(
        {
          event: 'tts_synthesized',
          duration_ms: ttsDuration,
          audio_bytes: result.audio.length,
          ...this.logContext,
        },
        'tts synthesized',
      );

      if (!this.active || this.playbackState.interrupted) {
        return;
      }

      this.logTtsBytesReady(turnId, result.audio, result.contentType);
      let playbackAudio = result.audio;
      const applyPstnPipeline = env.PLAYBACK_PROFILE === 'pstn' && this.transport.mode === 'pstn';
      if (applyPstnPipeline) {
        const endPipeline = startStageTimer('tts_pipeline_ms', tenantLabel);
        const pipelineResult = runPlaybackPipeline(playbackAudio, {
          targetSampleRateHz: env.PLAYBACK_PSTN_SAMPLE_RATE,
          enableHighpass: env.PLAYBACK_ENABLE_HIGHPASS,
          logContext: this.logContext,
        });
        endPipeline();
        playbackAudio = pipelineResult.audio;
      }
      if (applyPstnPipeline) {
        this.logWavInfo('pipeline_output', turnId, playbackAudio);
        const pipelineMeta = getAudioMeta(playbackAudio) ?? {
          format: 'wav' as const,
          logContext: { ...this.logContext, tts_id: turnId },
          lineage: ['pipeline:unknown'],
        };
        probeWav('tts.out.telephonyOptimized', playbackAudio, pipelineMeta);
      }
      result.audio = playbackAudio;

      const playbackInput =
        this.transport.mode === 'pstn'
          ? { kind: 'url' as const, url: await storeWav(this.callControlId, turnId, result.audio) }
          : { kind: 'buffer' as const, audio: result.audio, contentType: result.contentType };

      if (this.playbackState.interrupted) {
        return;
      }

      if (this.transport.mode === 'pstn' && this.shouldSkipTelnyxAction('playback_start')) {
        // ✅ beginPlayback already ran; do not leave state stuck in SPEAKING
        this.onPlaybackEnded();
        return;
      }

      const playbackStage = this.transport.mode === 'pstn' ? 'telnyx_playback' : 'webrtc_playback_ms';
      const endPlayback = startStageTimer(playbackStage, tenantLabel);

      const playbackStart = Date.now();
      try {
        if (this.transport.mode === 'pstn') {
          const txMeta = getAudioMeta(playbackAudio) ?? {
            format: 'wav' as const,
            logContext: { ...this.logContext, tts_id: turnId },
            lineage: ['tx:unknown'],
          };
          probeWav('tx.telnyx.payload', playbackAudio, { ...txMeta, kind: turnId });
        }

        markAudioSpan('tx_sent', spanMeta);
        await this.transport.playback.play(playbackInput);

        // ✅ always clear playback state when playback completes (single-turn playback)
        this.onPlaybackEnded();
      } catch (error) {
        incStageError(playbackStage, tenantLabel);

        // ✅ also clear playback state if playback throws
        this.onPlaybackEnded();

        throw error;
      } finally {
        endPlayback();
      }


      const playbackDuration = Date.now() - playbackStart;



      if (this.transport.mode === 'pstn') {
        log.info(
          {
            event: 'telnyx_playback_duration',
            duration_ms: playbackDuration,
            audio_url: (playbackInput as { kind: 'url'; url: string }).url,
            ...this.logContext,
          },
          'telnyx playback completed',
        );
      }
    } catch (error) {
      log.error({ err: error, ...this.logContext }, 'call session tts playback failed');
    } finally {
      // ✅ Do NOT force LISTENING here.
      // onPlaybackEnded() is the single source of truth for clearing playback + entering LISTENING.
      // But if we returned early (e.g. interrupted) and somehow stayed SPEAKING/active, clean up.
      if (this.playbackState.active || this.state === 'SPEAKING') {
        this.onPlaybackEnded();
      }
    }
  }


  private queueTtsSegment(segmentText: string, segmentId: string, handlingToken?: number): void {
    if (!segmentText.trim()) {
      return;
    }
    if (!this.active || this.state === 'ENDED') {
      return;
    }
    if (handlingToken !== undefined && handlingToken !== this.transcriptHandlingToken) {
      return;
    }

    if (!this.playbackState.active) {
      this.beginPlayback(segmentId);
    }
    this.ttsSegmentQueueDepth += 1;
    const queueDepth = this.ttsSegmentQueueDepth;

    log.info(
      {
        event: 'tts_segment_queued',
        seg_len: segmentText.length,
        queue_depth: queueDepth,
        segment_id: segmentId,
        ...this.logContext,
      },
      'tts segment queued',
    );

    this.ttsSegmentChain = this.ttsSegmentChain
      .then(async () => {
        await this.playTtsSegment(segmentText, segmentId);
      })
      .catch((error) => {
        log.error({ err: error, ...this.logContext }, 'tts segment playback failed');
      })
      .finally(() => {
        this.ttsSegmentQueueDepth = Math.max(0, this.ttsSegmentQueueDepth - 1);

        // ✅ Playback ends ONCE when all queued segments are done
        if (this.ttsSegmentQueueDepth === 0) {
          this.onPlaybackEnded();
        }
      });
  }

  private async playTtsSegment(segmentText: string, segmentId: string): Promise<void> {
    const shouldAbort = !this.active || this.state === 'ENDED' || this.playbackState.interrupted;
    if (shouldAbort) {
      return;
    }

    const tenantLabel = this.tenantId ?? 'unknown';
    const endTts = startStageTimer('tts', tenantLabel);

    const spanMeta = {
      callId: this.callControlId,
      tenantId: this.tenantId,
      logContext: { ...this.logContext, tts_id: segmentId },
      kind: segmentId,
    };
    markAudioSpan('tts_start', spanMeta);
    const ttsStart = Date.now();
    let result: TTSResult;
    try {
      result = await synthesizeSpeech({
        text: segmentText,
        voice: this.ttsConfig?.voice,
        format: this.ttsConfig?.format,
        sampleRate: this.ttsConfig?.sampleRate,
        kokoroUrl: this.ttsConfig?.kokoroUrl,
      });
    } catch (error) {
      incStageError('tts', tenantLabel);
      throw error;
    } finally {
      endTts();
    }
    const ttsDuration = Date.now() - ttsStart;
    markAudioSpan('tts_ready', spanMeta);


    log.info(
      {
        event: 'tts_synthesized',
        duration_ms: ttsDuration,
        audio_bytes: result.audio.length,
        ...this.logContext,
      },
      'tts synthesized',
    );

    if (!this.active || this.state === 'ENDED' || this.playbackState.interrupted) {
      return;
    }

    this.logTtsBytesReady(segmentId, result.audio, result.contentType);
    let playbackAudio = result.audio;
    const applyPstnPipeline = env.PLAYBACK_PROFILE === 'pstn' && this.transport.mode === 'pstn';
    if (applyPstnPipeline) {
      const endPipeline = startStageTimer('tts_pipeline_ms', tenantLabel);
      const pipelineResult = runPlaybackPipeline(playbackAudio, {
        targetSampleRateHz: env.PLAYBACK_PSTN_SAMPLE_RATE,
        enableHighpass: env.PLAYBACK_ENABLE_HIGHPASS,
        logContext: this.logContext,
      });
      endPipeline();
      playbackAudio = pipelineResult.audio;
    }
    if (applyPstnPipeline) {
      this.logWavInfo('pipeline_output', segmentId, playbackAudio);
      const pipelineMeta = getAudioMeta(playbackAudio) ?? {
        format: 'wav' as const,
        logContext: { ...this.logContext, tts_id: segmentId },
        lineage: ['pipeline:unknown'],
      };
      probeWav('tts.out.telephonyOptimized', playbackAudio, pipelineMeta);
    }
    result.audio = playbackAudio;

    const playbackInput =
      this.transport.mode === 'pstn'
        ? { kind: 'url' as const, url: await storeWav(this.callControlId, segmentId, result.audio) }
        : { kind: 'buffer' as const, audio: result.audio, contentType: result.contentType };

    if (this.playbackState.interrupted) {
      return;
    }

    if (this.transport.mode === 'pstn') {
      log.info(
        {
          event: 'tts_segment_play_start',
          seg_len: segmentText.length,
          segment_id: segmentId,
          audio_url: (playbackInput as { kind: 'url'; url: string }).url,
          ...this.logContext,
        },
        'tts segment playback start',
      );
    }

    const playbackStage = this.transport.mode === 'pstn'
      ? 'telnyx_playback'
      : 'webrtc_playback_ms';
    const endPlayback = startStageTimer(playbackStage, tenantLabel);

    const playbackStart = Date.now();
    try {
      if (this.transport.mode === 'pstn') {
        const txMeta = getAudioMeta(playbackAudio) ?? {
          format: 'wav' as const,
          logContext: { ...this.logContext, tts_id: segmentId },
          lineage: ['tx:unknown'],
        };
        probeWav('tx.telnyx.payload', playbackAudio, { ...txMeta, kind: segmentId });
      }

      markAudioSpan('tx_sent', spanMeta);
      await this.transport.playback.play(playbackInput);

      // ✅ IMPORTANT: do NOT call onPlaybackEnded() here.
      // Streaming playback ends when the segment queue drains.
    } catch (error) {
      incStageError(playbackStage, tenantLabel);

      // ✅ IMPORTANT: do NOT call onPlaybackEnded() here either.
      throw error;
    } finally {
      endPlayback();
    }


    const playbackDuration = Date.now() - playbackStart;



    if (this.transport.mode === 'pstn') {
      log.info(
        {
          event: 'tts_segment_play_end',
          seg_len: segmentText.length,
          segment_id: segmentId,
          duration_ms: playbackDuration,
          audio_url: (playbackInput as { kind: 'url'; url: string }).url,
          ...this.logContext,
        },
        'tts segment playback end',
      );
    }
  }

  private waitForTtsSegmentQueue(): Promise<void> {
    if (!this.playbackStopSignal) {
      return this.ttsSegmentChain;
    }
    return Promise.race([this.ttsSegmentChain, this.playbackStopSignal.promise]);
  }

  private findSentenceBoundary(text: string): number | null {
    const match = text.match(/[.!?](?=\s|$)/);
    if (!match || match.index === undefined) {
      return null;
    }
    return match.index + 1;
  }

  private selectSegmentEnd(text: string, targetChars: number): number {
    if (text.length <= targetChars) {
      return text.length;
    }
    const slice = text.slice(0, targetChars);
    const lastSpace = slice.lastIndexOf(' ');
    if (lastSpace >= Math.floor(targetChars * 0.6)) {
      return lastSpace;
    }
    return targetChars;
  }

  private nextTurnId(): number {
    this.turnSequence += 1;
    return this.turnSequence;
  }

  private shouldSkipTelnyxAction(action: string): boolean {
    if (this.transport.mode !== 'pstn') {
      return false;
    }
    if (this.active) {
      return false;
    }

    const event = action === 'playback_stop' ? 'playback_stop_skipped' : 'telnyx_action_skipped_inactive';
    log.warn(
      { event, action, ...this.logContext },
      'skipping telnyx action - call inactive',
    );
    return true;
  }
}
